# ðŸ“… Day 17: Model Monitoring & Prediction Logging

## ðŸŽ¯ Objective
To implement model monitoring by logging predictions along with input data and timestamps, enabling tracking of model behavior in a production-like environment.

---

## ðŸ› ï¸ What Was Implemented

- Loaded the trained machine learning model
- Logged model predictions automatically
- Stored:
  - Prediction timestamp
  - Input features
  - Model output
- Created a monitoring pipeline to support future:
  - Data drift detection
  - Model retraining
  - Performance tracking

---

## ðŸ“‚ Files & Folder Structure

ML_Project/
â”œâ”€â”€ models/
â”‚ â””â”€â”€ best_model.pkl
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample_input.csv
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ prediction_logs.csv
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ monitor.py
â”‚ â””â”€â”€ test_monitor.py

yaml
Copy code

---

## ðŸ“„ File Description

### `monitor.py`
- Loads the trained model
- Performs predictions
- Logs predictions with timestamps into a CSV file

### `test_monitor.py`
- Sends sample input data to the monitoring pipeline
- Verifies prediction logging functionality

### `prediction_logs.csv`
- Stores real-time prediction logs
- Acts as monitoring data for production analysis

---

## ðŸ” Monitoring Workflow

sample_input.csv
â†“
monitor.py
â†“
best_model.pkl
â†“
prediction
â†“
prediction_logs.csv

yaml
Copy code

---

## âœ… Outcome

- Model predictions are successfully logged
- Monitoring pipeline works without errors
- Project now includes a production-ready ML lifecycle component

---

## ðŸ§  Key Learning

> Deployment is not the final step â€” continuous monitoring is essential to ensure model reliability and performance over time.

---

## ðŸš€ Next Steps

- Detect data drift using logged predictions
- Implement automated model retraining
- Add alerting and performance metrics